{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5GbIt7eEe0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PubMed Metadata Retriever\n",
        "\n",
        "This script enriches a dataset containing PMIDs by retrieving the following metadata:\n",
        "- Title and abstract\n",
        "- Authors\n",
        "- MeSH terms\n",
        "- Publication date [edat]\n",
        "\n",
        "It uses the Bio.Entrez module from Biopython to interact with NCBI's E-utilities.\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages if not already installed\n",
        "try:\n",
        "    import pandas as pd\n",
        "    from Bio import Entrez\n",
        "except ImportError:\n",
        "    !pip install biopython pandas\n",
        "    import pandas as pd\n",
        "    from Bio import Entrez\n",
        "\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Configuration settings - MODIFY THESE AS NEEDED\n",
        "# ----------------------------------------------\n",
        "# IMPORTANT: Always provide your email when using Entrez\n",
        "EMAIL = \"put email address here\"  # Replace with your email address\n",
        "Entrez.email = EMAIL\n",
        "\n",
        "# Optional: Provide your API key if you have one (increases rate limits)\n",
        "# API_KEY = \"your_api_key_here\"\n",
        "# Entrez.api_key = API_KEY\n",
        "\n",
        "# Processing parameters\n",
        "BATCH_SIZE = 10  # Number of PMIDs to process per batch\n",
        "RATE_LIMIT_DELAY = 0.5  # Delay between API calls in seconds\n",
        "LIMIT = None  # Set to a number to limit processing (or None to process all)\n",
        "# ----------------------------------------------\n",
        "\n",
        "# Function to load PMIDs from a CSV file\n",
        "def load_pmids(file_path=None):\n",
        "    \"\"\"\n",
        "    Load PMIDs from a CSV file either from a local path or uploaded by the user.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        List of PMIDs as strings\n",
        "    \"\"\"\n",
        "    if file_path is None or not os.path.exists(file_path):\n",
        "        print(\"Please upload your CSV file containing PMIDs...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"No file was uploaded. Using a sample PMID for demonstration.\")\n",
        "            return [\"21383081\"]  # Example PMID\n",
        "\n",
        "        file_path = next(iter(uploaded))\n",
        "\n",
        "    try:\n",
        "        # Try reading with pandas\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if 'pmid' column exists\n",
        "        if 'pmid' not in df.columns:\n",
        "            print(f\"Warning: 'pmid' column not found. Available columns: {df.columns.tolist()}\")\n",
        "            print(\"Using the first column as PMIDs...\")\n",
        "            pmids = df.iloc[:, 0].astype(str).tolist()\n",
        "        else:\n",
        "            pmids = df['pmid'].astype(str).tolist()\n",
        "\n",
        "        # Remove any NaN values and convert to strings\n",
        "        pmids = [str(int(float(pmid))) for pmid in pmids if str(pmid).lower() != 'nan']\n",
        "\n",
        "        print(f\"Successfully loaded {len(pmids)} PMIDs\")\n",
        "        return pmids\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        print(\"Using a sample PMID for demonstration.\")\n",
        "        return [\"21383081\"]  # Example PMID\n",
        "\n",
        "# Function to fetch metadata for a single PMID\n",
        "def fetch_pubmed_metadata(pmid):\n",
        "    \"\"\"\n",
        "    Fetch metadata for a single PMID using NCBI's E-utilities\n",
        "\n",
        "    Args:\n",
        "        pmid: PubMed ID as a string\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing article metadata\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Fetch the article data\n",
        "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        if not record.get('PubmedArticle'):\n",
        "            return {\"pmid\": pmid, \"error\": \"No data found\"}\n",
        "\n",
        "        article = record['PubmedArticle'][0]\n",
        "\n",
        "        # Extract article metadata\n",
        "        metadata = {\n",
        "            \"pmid\": pmid,\n",
        "            \"title\": \"\",\n",
        "            \"abstract\": \"\",\n",
        "            \"authors\": [],\n",
        "            \"mesh_terms\": [],\n",
        "            \"publication_date\": \"\",\n",
        "            \"edat\": \"\"\n",
        "        }\n",
        "\n",
        "        # Extract title\n",
        "        article_data = article['MedlineCitation']['Article']\n",
        "        metadata[\"title\"] = article_data.get('ArticleTitle', '')\n",
        "\n",
        "        # Extract abstract\n",
        "        if 'Abstract' in article_data and 'AbstractText' in article_data['Abstract']:\n",
        "            abstract_parts = article_data['Abstract']['AbstractText']\n",
        "            if isinstance(abstract_parts, list):\n",
        "                # Handle structured abstracts\n",
        "                abstract_text = []\n",
        "                for part in abstract_parts:\n",
        "                    # Check if this is a labeled part with a NlmCategory attribute\n",
        "                    if hasattr(part, 'attributes') and 'NlmCategory' in part.attributes:\n",
        "                        label = part.attributes['NlmCategory']\n",
        "                        abstract_text.append(f\"{label}: {part}\")\n",
        "                    else:\n",
        "                        abstract_text.append(str(part))\n",
        "                abstract_text = \" \".join(abstract_text)\n",
        "            else:\n",
        "                abstract_text = str(abstract_parts)\n",
        "            metadata[\"abstract\"] = abstract_text\n",
        "\n",
        "        # Extract authors\n",
        "        if 'AuthorList' in article_data:\n",
        "            for author in article_data['AuthorList']:\n",
        "                if isinstance(author, dict):  # Ensure author is a dictionary\n",
        "                    author_info = {}\n",
        "                    if 'LastName' in author:\n",
        "                        author_info[\"last_name\"] = author['LastName']\n",
        "                    if 'ForeName' in author:\n",
        "                        author_info[\"fore_name\"] = author['ForeName']\n",
        "                    elif 'Initials' in author:  # Use initials if ForeName is not available\n",
        "                        author_info[\"fore_name\"] = author['Initials']\n",
        "\n",
        "                    # Extract affiliation\n",
        "                    if 'AffiliationInfo' in author and author['AffiliationInfo']:\n",
        "                        affiliations = []\n",
        "                        for affil in author['AffiliationInfo']:\n",
        "                            if 'Affiliation' in affil:\n",
        "                                affiliations.append(affil['Affiliation'])\n",
        "                        if affiliations:\n",
        "                            author_info[\"affiliation\"] = \"; \".join(affiliations)\n",
        "                    elif 'Affiliation' in author:  # For older format\n",
        "                        author_info[\"affiliation\"] = author['Affiliation']\n",
        "\n",
        "                    if author_info:  # Only add if we have some data\n",
        "                        metadata[\"authors\"].append(author_info)\n",
        "\n",
        "        # Extract MeSH terms\n",
        "        if 'MeshHeadingList' in article['MedlineCitation']:\n",
        "            for mesh in article['MedlineCitation']['MeshHeadingList']:\n",
        "                if 'DescriptorName' in mesh:\n",
        "                    descriptor = mesh['DescriptorName']\n",
        "                    # Check if descriptor is a string or has attributes\n",
        "                    if isinstance(descriptor, str):\n",
        "                        mesh_term = descriptor\n",
        "                    else:\n",
        "                        ui = descriptor.attributes.get('UI', '')\n",
        "                        term = descriptor\n",
        "                        mesh_term = f\"{ui}: {term}\" if ui else term\n",
        "                    metadata[\"mesh_terms\"].append(mesh_term)\n",
        "\n",
        "        # Extract publication date from Journal\n",
        "        if 'Journal' in article_data and 'JournalIssue' in article_data['Journal']:\n",
        "            if 'PubDate' in article_data['Journal']['JournalIssue']:\n",
        "                pub_date = article_data['Journal']['JournalIssue']['PubDate']\n",
        "                date_parts = []\n",
        "\n",
        "                # Handle different date formats\n",
        "                if 'Year' in pub_date:\n",
        "                    date_parts.append(pub_date['Year'])\n",
        "                    if 'Month' in pub_date:\n",
        "                        date_parts.append(pub_date['Month'])\n",
        "                        if 'Day' in pub_date:\n",
        "                            date_parts.append(pub_date['Day'])\n",
        "                elif 'MedlineDate' in pub_date:\n",
        "                    date_parts.append(pub_date['MedlineDate'])\n",
        "\n",
        "                metadata[\"publication_date\"] = \" \".join(date_parts)\n",
        "\n",
        "        # Try to get electronic publication date (edat) from PubmedData history\n",
        "        if 'PubmedData' in article and 'History' in article['PubmedData']:\n",
        "            for date_item in article['PubmedData']['History']:\n",
        "                if isinstance(date_item, dict) and 'PubStatus' in date_item.attributes:\n",
        "                    if date_item.attributes['PubStatus'] == 'pubmed':\n",
        "                        year = date_item.get('Year', '')\n",
        "                        month = date_item.get('Month', '')\n",
        "                        day = date_item.get('Day', '')\n",
        "                        if year:\n",
        "                            metadata[\"edat\"] = f\"{year}-{month}-{day}\" if month and day else year\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"Error fetching metadata for PMID {pmid}: {error_msg}\")\n",
        "        return {\"pmid\": pmid, \"error\": error_msg}\n",
        "\n",
        "# Function to process PMIDs in batches\n",
        "def process_pmids(pmids, batch_size=BATCH_SIZE, limit=LIMIT):\n",
        "    \"\"\"\n",
        "    Process PMIDs in batches to avoid overloading the NCBI API\n",
        "\n",
        "    Args:\n",
        "        pmids: List of PMIDs to process\n",
        "        batch_size: Number of PMIDs to process per batch\n",
        "        limit: Maximum number of PMIDs to process (None for all)\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing metadata for each PMID\n",
        "    \"\"\"\n",
        "    if limit is not None and limit < len(pmids):\n",
        "        pmids = pmids[:limit]\n",
        "        print(f\"Processing the first {limit} PMIDs...\")\n",
        "\n",
        "    total_pmids = len(pmids)\n",
        "    print(f\"Starting to process {total_pmids} PMIDs in batches of {batch_size}\")\n",
        "\n",
        "    all_metadata = []\n",
        "\n",
        "    for i in range(0, total_pmids, batch_size):\n",
        "        batch_end = min(i + batch_size, total_pmids)\n",
        "        batch = pmids[i:batch_end]\n",
        "\n",
        "        print(f\"\\nProcessing batch {i//batch_size + 1}/{(total_pmids + batch_size - 1)//batch_size} (PMIDs {i+1}-{batch_end}/{total_pmids})\")\n",
        "\n",
        "        batch_metadata = []\n",
        "        for pmid in batch:\n",
        "            print(f\"  Fetching metadata for PMID: {pmid}\")\n",
        "            metadata = fetch_pubmed_metadata(pmid)\n",
        "            batch_metadata.append(metadata)\n",
        "\n",
        "            # Add a small delay to respect NCBI's rate limits\n",
        "            time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "        all_metadata.extend(batch_metadata)\n",
        "\n",
        "        # Display progress\n",
        "        success_count = sum(1 for item in batch_metadata if \"error\" not in item or not item[\"error\"])\n",
        "        print(f\"  Batch completed: {success_count}/{len(batch)} successful\")\n",
        "\n",
        "    print(f\"\\nProcessing complete! Retrieved metadata for {len(all_metadata)} PMIDs\")\n",
        "    return all_metadata\n",
        "\n",
        "# Function to create a flattened dataframe for CSV output\n",
        "def create_flattened_dataframe(metadata_list):\n",
        "    \"\"\"\n",
        "    Create a flattened DataFrame from the metadata list for CSV export\n",
        "\n",
        "    Args:\n",
        "        metadata_list: List of metadata dictionaries\n",
        "\n",
        "    Returns:\n",
        "        Pandas DataFrame with flattened structure\n",
        "    \"\"\"\n",
        "    flattened_data = []\n",
        "\n",
        "    for item in metadata_list:\n",
        "        flat_item = {\n",
        "            \"pmid\": item[\"pmid\"],\n",
        "            \"title\": item.get(\"title\", \"\"),\n",
        "            \"abstract\": item.get(\"abstract\", \"\"),\n",
        "            \"publication_date\": item.get(\"publication_date\", \"\"),\n",
        "            \"edat\": item.get(\"edat\", \"\"),\n",
        "            \"authors_count\": len(item.get(\"authors\", [])),\n",
        "            \"mesh_terms_count\": len(item.get(\"mesh_terms\", [])),\n",
        "            \"error\": item.get(\"error\", \"\")\n",
        "        }\n",
        "\n",
        "        # Add first 5 authors if available\n",
        "        authors = item.get(\"authors\", [])\n",
        "        for i in range(min(5, len(authors))):\n",
        "            author = authors[i]\n",
        "            flat_item[f\"author{i+1}_last\"] = author.get(\"last_name\", \"\")\n",
        "            flat_item[f\"author{i+1}_fore\"] = author.get(\"fore_name\", \"\")\n",
        "            if \"affiliation\" in author:\n",
        "                flat_item[f\"author{i+1}_affiliation\"] = author[\"affiliation\"]\n",
        "\n",
        "        # Add first 10 MeSH terms if available\n",
        "        mesh_terms = item.get(\"mesh_terms\", [])\n",
        "        for i in range(min(10, len(mesh_terms))):\n",
        "            flat_item[f\"mesh{i+1}\"] = mesh_terms[i]\n",
        "\n",
        "        flattened_data.append(flat_item)\n",
        "\n",
        "    return pd.DataFrame(flattened_data)\n",
        "\n",
        "# Function to save results and provide download links\n",
        "def save_and_download_results(metadata, base_filename=\"pubmed_metadata\"):\n",
        "    \"\"\"\n",
        "    Save results to CSV and JSON files and provide download links\n",
        "\n",
        "    Args:\n",
        "        metadata: List of metadata dictionaries\n",
        "        base_filename: Base filename for the output files\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Create DataFrame for CSV export\n",
        "    df = create_flattened_dataframe(metadata)\n",
        "\n",
        "    # Save CSV\n",
        "    csv_filename = f\"{base_filename}.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Saved CSV file: {csv_filename}\")\n",
        "\n",
        "    # Save full JSON\n",
        "    json_filename = f\"{base_filename}_full.json\"\n",
        "    with open(json_filename, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"Saved full JSON file: {json_filename}\")\n",
        "\n",
        "    # Provide download links\n",
        "    print(\"\\nDownload the results by running the following commands in separate cells:\")\n",
        "    print(f\"from google.colab import files\")\n",
        "    print(f\"files.download('{csv_filename}')\")\n",
        "    print(f\"files.download('{json_filename}')\")\n",
        "\n",
        "# Main function to run the entire process\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the entire process\n",
        "    \"\"\"\n",
        "    print(\"PubMed Metadata Retriever for Google Colab\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Using email: {EMAIL}\")\n",
        "    print(f\"Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"Rate limit delay: {RATE_LIMIT_DELAY} seconds\")\n",
        "    print(f\"Processing limit: {'All' if LIMIT is None else LIMIT} PMIDs\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Step 1: Load PMIDs\n",
        "    pmids = load_pmids()\n",
        "\n",
        "    if not pmids:\n",
        "        print(\"No PMIDs found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Process PMIDs\n",
        "    metadata = process_pmids(pmids, BATCH_SIZE, LIMIT)\n",
        "\n",
        "    # Step 3: Save and provide download links\n",
        "    save_and_download_results(metadata)\n",
        "\n",
        "    print(\"\\nProcess completed successfully!\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}